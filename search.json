[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "FLA 2026 Lecture Notes",
    "section": "",
    "text": "Preface\nThis book is a collection of lecture notes for COMS3003A and COMS3021A.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "lectures/lecture_1.html",
    "href": "lectures/lecture_1.html",
    "title": "1  An Introduction to Theoretical Computer Science",
    "section": "",
    "text": "Maybe one day I will write something for this page…",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An Introduction to Theoretical Computer Science</span>"
    ]
  },
  {
    "objectID": "lectures/lecture_2.html",
    "href": "lectures/lecture_2.html",
    "title": "2  Data, Representations, and Problems",
    "section": "",
    "text": "2.1 Data",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data, Representations, and Problems</span>"
    ]
  },
  {
    "objectID": "lectures/lecture_2.html#data",
    "href": "lectures/lecture_2.html#data",
    "title": "2  Data, Representations, and Problems",
    "section": "",
    "text": "2.1.1 Primitive Data\nSymbols: The most basic unit - may be anything that can be Read & Written\n\nNumbers: \\(0, 1\\)\nLatin Letters: \\(A, B, C, ..., Y, Z\\)\nGreek Letters: \\(\\alpha, \\beta\\)\nElectricity: \\(1.5V\\), \\(3.0V\\)\nSound\n\nAlphabet: A finite set of symbols\n\nCommon notation \\(\\Sigma\\), \\(\\Gamma\\), \\(\\Delta\\)\n\nStrings / Words:\n\nA finite sequence of symbols\n\\(\\varepsilon\\) is the empty word\nWords have a length \\(|w| = n \\iff \\text{there are } n \\text{ symbols in } w\\)\n\n\\(|\\varepsilon| = 0\\)\n\n\nString Operators:\n\nConcatenation - connecting two symbols or strings\n\n\\(w_1 \\circ w_2 = w_1 w_2\\)\n\nRepeat Concatenation - concatenate the same string the specified number of times\n\n\\(w^n = w \\circ w \\circ w \\:\\circ\\: ... \\:\\circ\\: w\\)\n\n\n\n\n2.1.2 Describing Strings\nString Generators: Describe a set of strings\n\nUnion - Either string\n\n\\(w \\cup v = \\{\\; w, v \\;\\}\\)\n\nKleene Star - Repeat the string any numbers of times\n\n\\(w^\\ast = \\{\\; \\varepsilon, w, ww, www, wwww, ... \\;\\}\\)\n\nKleene Plus - repeat the string at least once\n\n\\(w^+ = w w^\\ast =\\{\\; w, ww, www, ... \\;\\}\\)\n\nOtherwise called a regular expression\n\nThe set of all strings over an alphabet is \\[ \\Sigma^\\ast \\]\n\n\n\n\n\n\nNote\n\n\n\nLet \\(\\Sigma = \\{\\: a , b \\:\\}\\) \\[\\Sigma^\\ast = \\left\\{\\: \\begin{matrix} \\varepsilon, \\\\ a, aa, aaa, ... \\\\ b, bb, bbb, ... \\\\ ab, aba, abaa, ... \\\\ abba, abbaa, abbaaa, ... \\end{matrix} \\:\\right\\}\\]\n\n\n\n\n2.1.3 Languages\nA language \\(L\\) is a set of strings (i.e. a subset of \\(\\Sigma^\\ast\\))\n\n\n\n\n\n\nTipNotation\n\n\n\nAs shorthand, we may specify a set \\(L\\) is a language by indicating that it is a subset of all strings. \\[ L \\subseteq \\Sigma^\\ast \\]\n\n\nLanguage Examples:\n\nThe set of all strings with two symbols \\[ \\{ w \\in \\Sigma^\\ast \\mid |w| = 2 \\} \\]\nThe set of all strings that end in 0 \\[ \\{ w \\in \\Sigma^\\ast \\mid w = v \\circ u \\text{ and } u = 0 \\} \\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data, Representations, and Problems</span>"
    ]
  },
  {
    "objectID": "lectures/lecture_2.html#representations",
    "href": "lectures/lecture_2.html#representations",
    "title": "2  Data, Representations, and Problems",
    "section": "2.2 Representations",
    "text": "2.2 Representations\nA representation is an interpretation of objects into strings\nIf I give you 5 objects, how would you label them? How about 10 objects? What about an infinite number of objects?\nFormally, a representation is a mapping from a set of objects \\(X\\) to a string \\[ r: X \\mapsto \\Sigma^\\ast\\]\nWe also want to add the following properties to our representation:\n\nEvery input has some output \\[ \\forall x \\in X, \\exists w \\in \\Sigma^\\ast, r(x) = w \\]\nEvery output has at least one input \\[ \\forall w \\in \\Sigma^\\ast, \\exists x \\in X, r(x) = w \\]\nIf two inputs are different, their outputs must be different \\[ \\forall x,y \\in X,  x \\neq y \\implies r(x) \\neq r(y) \\]\n\n\n\n\n\n\n\nNoteA brief interlude on relations\n\n\n\nA relation \\(\\alpha: X \\mapsto Y\\) is the more general idea of a function. A function \\(f: X \\mapsto Y\\) requires that each \\(x \\in X\\) maps to only one \\(y \\in Y\\). Relations are a relaxation of that to-one property (called functional). Our first property of all \\(x \\in X\\) having at least one \\(y \\in Y\\) is called serial.\nAnother way to think of relations is using sets. A binary Relation \\(R: X \\mapsto Y\\) is the subset \\(R \\subseteq X \\times Y\\). Notationally, we say \\(x\\:R\\:y\\) (\\(x\\) relates to \\(y\\) by \\(R\\)) if \\((x,y) \\in R\\)\nAn example is equality of numbers \\(=: \\mathbb{R} \\mapsto \\mathbb{R}\\) \\[\\forall x, y \\in \\mathbb{R}, x = y \\iff (x,y) \\in \\{\\:\\ (a,a) \\mid a \\in \\mathbb{R} \\:\\}\\]\n\n\n\n\n\n\n\n\nTipNotation\n\n\n\nInstead of writing \\(r(x)\\) we will write \\(\\left&lt; x \\right&gt;_r\\) to mark it as a string\nIf it is clear what representation we are using, then we can drop the \\(r\\)\n\n\nA representation allows us to take random objects and ‘insert’ them into strings, but we also want to be able to ‘extract’ an object out of a string.\nSo we add the additional requirement that our representation must be invertible. We define some default value for \\(X\\). If the string cannot be identified, we just map it to this default value.\n\n\n\n\n\n\nWarning\n\n\n\nWe are hand-waving away a fairly important problem. Two strings may represent the same object, for example in decimal numbers: 0.99999… = 1\nAs an exercise, try write a more rigorous definition of a representation using a tool you were taught in abstract maths (hint: equivalence)\n\n\n\n2.2.1 Natural Numbers \\(\\mathbb{N}\\) in an alphabet of \\(k\\) symbols\n\\[\n    \\forall n \\in \\mathbb{N}, \\left&lt; n \\right&gt;_{\\mathbb{N}, k} = \\text{The base-} k \\text{ encoding of } n\n\\]\nBy default, we will use \\(k = 2\\). If the \\(k\\) is left off, then assume it is 2\n\n\n2.2.2 Integers \\(\\mathbb{Z}\\) in an alphabet of \\(k\\) symbols:\n\\[\\forall z \\in \\mathbb{Z}, \\left&lt; z \\right&gt;_{\\mathbb{Z}, k} = \\begin{cases}\n    0 \\circ \\left&lt;|z|\\right&gt;_{\\mathbb{N}, k} \\text{ if } z \\geq 0 \\\\\n    1 \\circ \\left&lt;|z|\\right&gt;_{\\mathbb{N}, k} \\text{ if } z &lt; 0\n\\end{cases}\\]\n\n\n\n\n\n\nWarning\n\n\n\nWhat does the string \\(00\\) mean in this representation?\nThis is where our “default” value for a “garbage” string comes into play. We cannot recogize \\(00\\) according to our rules, so we instead just map it to the integer \\(0\\).\n\n\n\n\n2.2.3 Rationals \\(\\mathbb{Q}\\) in an alphabet of \\(k\\) symbols\n\\[ \\forall \\frac{x}{y} \\in \\mathbb{Q}, \\left&lt;\\frac{x}{y}\\right&gt;_{\\mathbb{Q},k} = \\left&lt;x\\right&gt;_{\\mathbb{Z}, k}\\#\\left&lt;y\\right&gt;_{\\mathbb{Z},k}\\] We’ve had to add the symbol \\(\\#\\) to separate our integers in the fraction. We can simplify this down into just our alphabet of \\(k\\) symbols.\nTo pull a rational number out of a string, we use the reverse of the above, but if we cannot recognize the string, we instead default to \\(0\\).\nBinary simplifier example: \\[ e_2(x) = \\begin{cases}\n    00 \\quad\\text{ if } x = 0 \\\\\n    01 \\quad\\text{ if } x = 1 \\\\\n    10 \\quad\\text{ if } x = \\# \\\\\n    11 \\quad\\text{ otherwise }\n\\end{cases}\n\\]\n\n\n2.2.4 Booleans\n\\[\n    \\left&lt; b \\right&gt;_{\\mathbb{B},k} = \\begin{cases}\n        1 & \\text{if } b = \\text{True} \\\\\n        0 & \\text{if } b = \\text{False}\n    \\end{cases}\n\\]\n\n\n2.2.5 Complex Objects\nWe know have a fairly robust set of primitive representations. We can encode natural numbers, integers, rationals, and booleans, but how can we encode a more complex object? One that is made up of several sub-objects?\nIf our object has 2 components (imagine a 2D vector), we can employ the same separation as our rationals.\n\\[ \\left&lt; \\left(\\begin{matrix}x\\\\ y\\end{matrix}\\right) \\right&gt; := \\left&lt;x\\right&gt; \\# \\left&lt;y\\right&gt; \\]\nFor \\(m\\) components? \\[ \\left&lt; \\left(\\begin{matrix} a_1\\\\ a_2 \\\\ \\vdots \\\\ a_m \\end{matrix}\\right) \\right&gt; := \\left&lt;a_1\\right&gt; \\# \\left&lt;a_2\\right&gt; \\# ... \\# \\left&lt;a_m\\right&gt;\\]\n\n\n\n\n\n\nWarningWhat if we have a vector of rationals?\n\n\n\nWe use \\(\\#\\) as a separator for both our vector components and our numerator and denominator. This is a collision of meaning. How can we write something like \\[ \\begin{pmatrix} \\frac{a}{b} \\\\ \\frac{c}{d} \\end{pmatrix}\\] Our current encoding of this just becomes \\[\\left&lt;\\begin{pmatrix} \\frac{a}{b} \\\\ \\frac{c}{d} \\end{pmatrix}\\right&gt; = \\left&lt;a\\right&gt;\\#\\left&lt;b\\right&gt;\\#\\left&lt;c\\right&gt;\\#\\left&lt;d\\right&gt;\\] which is either a 4D integer vector, a 3D vector in \\(\\mathbb{Z} \\times \\mathbb{Q} \\times \\mathbb{Z}\\), or a 2D vector of rationals.\nWe can replace the vector separator \\(\\#\\) with either a new symbol \\(;\\) or an additional \\(\\#\\) \\[\\left&lt;\\begin{pmatrix} \\frac{a}{b} \\\\ \\frac{c}{d} \\end{pmatrix}\\right&gt; = \\left&lt;a\\right&gt;\\#\\left&lt;b\\right&gt;;\\left&lt;c\\right&gt;\\#\\left&lt;d\\right&gt;\\]\nTo get back to Binary alphabet we can use the following encoding: \\[\ne(s) = \\begin{cases}\n    00 \\; s = 0 \\\\\n    01 \\; s = 1 \\\\\n    10 \\; s = \\# \\\\\n    11 \\; s = \\; ;\n\\end{cases}\n\\]\n\n\n\n\n\n\n\n\nWarningWhat is the smallest alphabet?\n\n\n\nWhat is the smallest possible alphabet we can construct? How might this alphabet represent different objects?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data, Representations, and Problems</span>"
    ]
  },
  {
    "objectID": "lectures/lecture_2.html#encodings",
    "href": "lectures/lecture_2.html#encodings",
    "title": "2  Data, Representations, and Problems",
    "section": "2.3 Encodings",
    "text": "2.3 Encodings\nEncodings are the technique we use to write symbols in a different alphabet. We apply them on a semi-regular basis in computer science.\nAn encoding is a mapping that converts a symbol to a string in another alphabet \\[e: \\Sigma \\mapsto L \\subseteq \\Gamma^*\\] \\[s \\in \\Sigma \\implies e(s) \\in L \\subseteq \\Gamma^\\ast\\]\n\n\n\n\n\n\nNoteNotation\n\n\n\nWe can also “encode” strings\n\\[\\begin{align*}\n    \\text{Let } &w = s_1 s_2 ... s_n \\in \\Sigma^\\ast \\text{ and } e: \\Sigma \\mapsto L \\subseteq \\Gamma^\\ast \\\\\\\\\n     & e(w) = \\left&lt; w\\right&gt;_e = e(s_1) e(s_2) ... e(s_n) \\in \\Gamma^\\ast\n\\end{align*}\\]\n\n\nThere are two types of encodings:\n\nFixed Width: All encoded strings are the same length\n\nA new symbol is indicated by reaching the end of the fixed length\nExample: ASCII \\[ e: \\Sigma \\mapsto \\Gamma^n \\]\n\nVariable Width: All encoded strings have different lengths \\[ e: \\Sigma \\mapsto \\{ w \\in \\Gamma^\\ast \\mid |w| \\leq n \\} \\]\n\nA new symbol is indicated in some fashion\n\nAll symbols start the same way, but feature different symbols for the rest\nA space between symbols\n\nExample: Morse Code\n\nMorse Code was originally designed as an auditory alphabet. It has three symbols, a short sound (dot), a long sound (dash), and no sound (seperator). However, we can simplify it into a binary alphabet of sound, no sound by defining a dash as the repetition of sound over a fixed time interval (the standard is 3 times the length of a dot).\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nAll encodings are representations, just from an alphabet of symbols \\(\\Sigma\\) to strings \\(\\Gamma^\\ast\\) instead of a arbitrary set \\(X\\). Can we make an encoding from a set of strings to another set of strings? \\[ e: \\Sigma^\\ast \\mapsto \\Gamma^\\ast \\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data, Representations, and Problems</span>"
    ]
  },
  {
    "objectID": "lectures/lecture_2.html#problems",
    "href": "lectures/lecture_2.html#problems",
    "title": "2  Data, Representations, and Problems",
    "section": "2.4 Problems",
    "text": "2.4 Problems\nWe want to draw a distinction between an algorithm and a problem.\nWe define a problem, as the class of inputs, and class of outputs for which we are interested in.\n\\[ P: X \\mapsto Y \\]\nAn algorithm is the set of instructions by which we manipulate our inputs into our outputs.\nA good intuition for the distinction is that algorithms can call other algorithms, but they cannot call a problem.\n\n\n\n\n\n\nNote\n\n\n\nTo rely on some prior programming experience, you can imagine that a problem is a “declaration” in code, while the algorithm is the actual “definition”.\nFor example, in C++, in some header (.h) file we write\nunsigned int compute_square(unsigned int n);\nwhich is our “Problem” \\(\\text{COMPUTE\\_SQUARE}: \\mathbb{N} \\mapsto \\mathbb{N}\\).\nand then in a .cpp file we have\nunsigned int compute_square(unsigned int n) {\n    unsigned int n_square = n * n;\n    return n_square;\n}\nas our algorithm.\nThis is not a perfect one-to-one relationship. Our header definition lacks the description of what our output should look like as the full definition of \\(\\text{COMPUTE\\_SQUARE}\\) is \\[ \\text{COMPUTE\\_SQUARE}(n) = n^2 \\] and implementing it in code is itself an almost verbatim description of the problem. Use it as a starting off position.\n\n\nExamples:\n\nPrime Factorisation: Input any natural, output: list of prime numbers \\[P: \\mathbb{N} \\mapsto [\\mathbb{N}]\\]\nDerivative: Input a differentiable function, output the derivative function \\[P: F \\mapsto F^\\prime \\]\nSwap Two Numbers: Input two naturals, output two naturals swapped \\[P: \\mathbb{N} \\times \\mathbb{N} \\mapsto \\mathbb{N} \\times \\mathbb{N}\\]\n\nWe want to “compute” the answer, so we turn the domain and co-domain into a string with representations \\(r_X\\) and \\(r_Y\\) that we can manipulate \\[ P: r_X(X) \\mapsto r_Y(Y) \\]\nA Computational Problem is therefore a function \\(f\\) from one set of strings \\(\\Sigma^\\ast\\) to another set of strings \\(\\Gamma^\\ast\\)\n\\[f: \\Sigma^\\ast \\mapsto \\Gamma^\\ast\\]\nThe most common form however is\n\\[f: \\{\\;0, 1\\;\\}^\\ast \\mapsto \\{\\;0, 1\\;\\}^\\ast\\]\nSometimes, we just want to identify if a given object has some property attached to it\n\nIs this number even?\nIs it a rational?\nCan I take the derivative?\n\nWe call these kinds of a problems a Decision Problem\n\\[f: \\Sigma^\\ast \\mapsto \\{\\; 0, 1 \\;\\}\\]\nIn fact, we can restructure any computational problem into a decision problem.\n\n2.4.1 Decision Problem and Computational Problem Equivalence\nAs an example: Lets take the computation problem of dividing a natural number\n\\[\\begin{align*}\n    \\text{Let } n,m \\in \\mathbb{N}, \\text{ and } \\frac{n}{m} \\in \\mathbb{Q} \\\\\n    \\text{COMPUTE\\_NATURAL\\_DIV}(n,m) &= \\frac{n}{m}\n\\end{align*}\\]\nLet assume an algorithm for \\(\\text{COMPUTE\\_NATURAL\\_DIV}\\) exists, say \\(A\\).\nIf we give \\(A\\) a string natural number pair \\(\\left&lt; n, m \\right&gt;\\) it will output the string \\(\\left&lt; \\frac{n}{m} \\right&gt;\\)\nWe define the Decision problem with \\(n,m,q \\in \\mathbb{N}\\) \\[\\text{IS\\_NATURAL\\_DIV}(n,m,q) = \\begin{cases}\n    1 \\text{ if } \\frac{n}{m} = q \\\\\n    0 \\text{ if } \\frac{n}{m} \\neq q\n\\end{cases}\\]\nLet \\(A\\) be an algorithm that solves \\(\\text{COMPUTE\\_NATURAL\\_DIV}\\).\nWe write a new algorithm \\(B\\) on that solves \\(\\text{IS\\_NATURAL\\_DIV}\\)\ndefine \\(B\\) on input \\(\\left&lt;n,m,q\\right&gt;\\)\n\nIf \\(A(\\left&lt;n,m\\right&gt;) = q\\)\n\nTrue =&gt; Return 1\nFalse =&gt; Return 0\n\n\nAnd now for the reverse:\nLet \\(B\\) be an algorithm that solves \\(\\text{IS\\_NATURAL\\_DIV}\\)\ndefine \\(A\\) on input \\(\\left&lt;n,m\\right&gt;\\):\n\nfor \\(q\\) in \\([1, n]\\)\n\nif \\(B(\\left&lt;n,m,q\\right&gt;) = 1\\)\n\nTrue =&gt; return q\nFalse =&gt; do nothing\n\n\nreturn \\(\\left&lt;0\\right&gt;\\)\n\nAlgorithm \\(A\\) solves \\(\\text{COMPUTE\\_NATURAL\\_DIV}\\) as:\n\nIf \\(n &lt; m\\) then \\(\\frac{n}{m} \\notin \\mathbb{N}\\) and no answer exists or \\(n = 0\\) therefore the loop will never have a \\(q\\) with \\(B(\\left&lt;n,m,q\\right&gt;) = 1\\), thus we return \\(\\left&lt; 0\\right&gt;\\)\nIf \\(n = m\\) then \\(\\frac{n}{m} = 1\\), therefore for \\(q = 1\\), we will return \\(q\\)\nIf \\(n &gt; m\\) then \\(n = qm\\)\n\nThe smallest possible \\(q\\) is \\(1\\) and \\(m = n\\)\nThe largest possible \\(q\\) value is \\(q = n\\) and occurs when \\(m = 1\\)\nIf \\(q &gt; n\\) then \\(m &lt; 1\\), thus \\(m \\notin \\mathbb{N}\\), therefore \\(B\\) does not apply\n\n\n\n\n\n\n\n\nWarning\n\n\n\nTry to do the same for a computational problem \\(P\\). Given \\(P: \\{0,1\\}^\\ast \\mapsto \\{0,1\\}^\\ast\\) and algorithm \\(A_P\\), show there exists a decision problem \\(P^\\prime: \\{0,1\\}^\\ast \\mapsto \\{0,1\\}\\)\n\n\n\n\n2.4.2 Language and Decision Problem Equivalence\nLanguages are equivalent to decision problems. We can construct a decision problem out of language membership, and we can construct language membership out of a decision problem.\n\\[ w \\in L \\iff D(w) = 1 \\]\n\n\n\n\n\n\nNote\n\n\n\nShow \\(D(w) = 1 \\implies w \\in L\\)\n\nLet \\(D: \\Sigma^\\ast \\mapsto \\{0, 1\\}\\) be a decision problem.\nDefine the language \\(L\\) as \\[ L = \\{ w \\in \\Sigma^\\ast \\mid D(w) = 1 \\} \\]\n\nShow \\(w \\in L \\implies D(w) = 1\\)\n\nLet \\(L \\subseteq \\Sigma^\\ast\\) be a language.\nDefine the decision problem \\(D\\) as \\[ D(w) = \\begin{cases} 1 & w \\in L \\\\ 0 & w \\notin L \\end{cases} \\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data, Representations, and Problems</span>"
    ]
  },
  {
    "objectID": "lectures/lecture_3.html",
    "href": "lectures/lecture_3.html",
    "title": "3  Turing Machines I",
    "section": "",
    "text": "3.1 A machine that computes\nYour intuition might be that a computer is a small box that sits on a desk, in a server rack, and even in your pocket. Making use of electronics, digital circuits, and millions of bits and bytes to perform what is sufficiently indistinguishable from magic 1.\nIn 1936, they had a very different picture. The closest equivalent were mechanical devices that used levers, linkages, and gears to produce work - similar to a train. Instead of work being motion, it would be the evaluation a series of steps, generally finding a solution to some form of arithmetic. Mechanical calculators were probably the most well known instances of a “computer”.\nWhen we build our mathematical model of a computer and algorithm, I want you to keep in mind the ‘mechanical’ nature of it.\nFor our model to be correct, there are important ideas that need to be captured:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Turing Machines I</span>"
    ]
  },
  {
    "objectID": "lectures/lecture_3.html#a-machine-that-computes",
    "href": "lectures/lecture_3.html#a-machine-that-computes",
    "title": "3  Turing Machines I",
    "section": "",
    "text": "Note\n\n\n\nThe first general purpose computer was actually proposed about 100 years before Turing in 1837. Charles Babbage designed a machine he called the “Analytical Engine”.\nThis machine was a digital-mechanical hybrid that made use of components that would not be out of place in a contemporary digital computer. Had it been finished, it would have been the first known instance of device that is Turing Complete 2.\nAda Lovelace is often called the first programmer because she wrote the first published progam for this device. 3 This program, called Note G, calculated a sequence of Bernoulli Numbers.\n\n\n\n\n\nIt should be as simple as possible\n\nWhat is the smallest amount of ‘work’ we can get away with for each step in our device?\nWe will use a string as the only data structure\n\nIt should be able to do everything an algorithm can do - For loops, if statements, variables etc. should all be implementable on the device",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Turing Machines I</span>"
    ]
  },
  {
    "objectID": "lectures/lecture_3.html#turing-machines",
    "href": "lectures/lecture_3.html#turing-machines",
    "title": "3  Turing Machines I",
    "section": "3.2 Turing Machines",
    "text": "3.2 Turing Machines\n\n3.2.1 Some intuition\nImagine a physical device that has three components:\n\nA space to read and write symbols - a tape of sequential cells\nA mechanism to perform the read and write - a head that can read or write on a given cell on our tape\nA control unit to tell the the head what symbol it should be writing - a box that keeps track of our algorithm steps and where along that algorithm where are\n\n\nOn our tape, we have a set of symbols on every cell - even if that symbol is a “blank” (\\(\\sqcup\\)) symbol to represent nothing (\\(\\sqcup, 0, 1, 2, ...\\)).\nWe will also assume that our tape has as much space as it needs - i.e. an infinite tape.\nOur head mechanism, can move between the different cells going either left or right along it.\nThe control unit is composed of various ‘device states’ (\\(q_1, q_2, q_3, ...\\)) that holds the information about whats happened so far (Gears in different positions, or electronic signals in specific patterns).\n\nThis device has only one instruction: A conditional statement based off the current device state \\(q_i\\), and the symbol currently on the tape \\(s_n\\) that tells us what state to go to next \\(q_j\\), what symbol should be written on the tape \\(s_m\\), and whether we should go left or right \\(D \\in \\{ L, R \\}\\).\n\\[ q_i, s_n \\rightarrow q_j, s_m, D \\]\n\n\n\n\n\n\nNote\n\n\n\nIf we tried to build this device in the real world, these instructions are just the series of levers and gears that interact to create our desired effect.\n\nWrite this symbol - rotate a ball with all the symbols on it\nChange to the 5th state - rotate this gear by 5 teeth from this notch\nGo left - rotate this other gear counter-clockwise\n\n\n\n\nWe will add the stipulation that if our machine enters specific states, the computation will stop: For a Yes / No (Accept / Reject) machine we will have the special states \\(q_\\text{accept}, q_\\text{reject}\\) indicating the corresponding answer.\nAdditionally, we need to start somewhere, so we designate \\(q_0\\) as the ‘start state’. If we need to run this device, we must first reset it so that we are in \\(q_0\\).\nOur programming language for this device will be all the possible permutations of these instructions that direct the device:\n\nIn \\(q_0\\),\n\nreading \\(\\sqcup\\), go to \\(\\{ q_0, q_1, q_\\text{halt} \\}\\), writing symbol \\(\\{ \\sqcup, 0, 1 \\}\\) and moving \\(\\{ L, R \\}\\).\nreading \\(0\\), go to \\(\\{ q_0, q_1, q_\\text{halt} \\}\\), writing symbol \\(\\{ \\sqcup, 0, 1 \\}\\) and moving \\(\\{ L, R \\}\\).\nreading \\(1\\), go to \\(\\{ q_0, q_1, q_\\text{halt} \\}\\), writing symbol \\(\\{ \\sqcup, 0, 1 \\}\\) and moving \\(\\{ L, R \\}\\).\n\nIn \\(q_1\\),\n\nreading \\(\\sqcup\\), go to \\(\\{ q_0, q_1, q_\\text{halt} \\}\\), writing symbol \\(\\{ \\sqcup, 0, 1 \\}\\) and moving \\(\\{ L, R \\}\\).\nreading \\(0\\), go to \\(\\{ q_0, q_1, q_\\text{halt} \\}\\), writing symbol \\(\\{ \\sqcup, 0, 1 \\}\\) and moving \\(\\{ L, R \\}\\).\nreading \\(1\\), go to \\(\\{ q_0, q_1, q_\\text{halt} \\}\\), writing symbol \\(\\{ \\sqcup, 0, 1 \\}\\) and moving \\(\\{ L, R \\}\\).\n\n\nOur program \\(\\delta\\) is the subset of of these permutations that describe the particular behaviour we want.\n\nIn \\(q_0\\)\n\nreading \\(\\sqcup\\), go to \\(q_1\\), writing \\(\\sqcup\\) and moving \\(L\\)\nreading \\(0\\), go to \\(q_0\\), writing \\(0\\) and moving \\(R\\)\nreading \\(1\\), go to \\(q_0\\), writing \\(1\\) and moving \\(R\\)\n\nIn \\(q_1\\)\n\nreading \\(\\sqcup\\), go to \\(q_\\text{reject}\\), writing \\(\\sqcup\\) and moving \\(R\\)\nreading \\(0\\), go to \\(q_\\text{reject}\\), writing \\(0\\) and moving \\(R\\)\nreading \\(1\\), go to \\(q_\\text{accept}\\), writing \\(1\\) and moving \\(R\\)\n\n\n\n\n\n\n\n\n\nWarningQuestion\n\n\n\nWhat do these instructions actually do? Try describe the above algorithm in english.\nIf we use the binary repesentation of integer’s, \\(\\left&lt; z \\right&gt;_{\\mathbb{Z}}\\), what is happening to the number \\(z \\in \\mathbb{Z}\\)?\n\n\nAnd a computation is the series of changes that has occured in our device and tape.\n\nSet the tape to be the input string \\(0111\\),\nSet the device to be in \\(q_0\\) and the head to be at the first symbol\n\nWe always start at the first symbol of our input\n\nFollow each instruction until \\(q_\\text{accept}\\) or \\(q_\\text{reject}\\)\n\nStep \\(0\\): In \\(q_0\\), scanning cell \\(0\\), reading symbol \\(0\\), go to \\(q_0\\), write \\(0\\), move right to cell \\(1\\)\n\n\\(q_0 0111\\)\n\nStep \\(1\\): In \\(q_0\\), scanning cell \\(1\\), reading symbol \\(1\\), go to \\(q_0\\), write \\(1\\), move right to cell \\(2\\)\n\n\\(0 q_0 111\\)\n\nStep \\(2\\): In \\(q_0\\), scanning cell \\(2\\), reading symbol \\(1\\), go to \\(q_0\\), write \\(1\\), move right to cell \\(3\\)\n\n\\(01 q_0 11\\)\n\nStep \\(3\\): In \\(q_0\\), scanning cell \\(3\\), reading symbol \\(1\\), go to \\(q_0\\), write \\(1\\), move right to cell \\(4\\)\n\n\\(011 q_0 1\\)\n\nStep \\(4\\): In \\(q_0\\), scanning cell \\(4\\), reading symbol \\(\\sqcup\\), go to \\(q_1\\), write \\(\\sqcup\\), move left to cell \\(3\\)\n\nWe have moved off the end of our string into a “blank” cell\n\\(0111 q_0 \\sqcup\\)\n\nStep \\(5\\): In \\(q_1\\), scanning cell \\(3\\), reading symbol \\(1\\), go to \\(q_\\text{accept}\\), write \\(1\\), move right to cell \\(4\\)\n\n\\(011 q_1 1\\)\n\nStep \\(6\\): In \\(q_\\text{accept}\\) scanning cell \\(4\\) \\(\\rightarrow\\) halt\n\n\\(0111 q_\\text{accept} \\sqcup\\)\n\n\nRead the result as \\(1\\) as we are in \\(q_\\text{accept}\\)\n\nCongratulations, we have just built a physical device that evaluates some algorithm. If we use \\(\\left&lt; z \\right&gt;_{\\mathbb{Z}}\\), then this algorithm determines if the given integer is even or odd.\n\n\n3.2.2 Formal Definition\nThe previous device is essentially an implementation of a Turing Machine as a mechanical machine.\nMathematically, a Turing Machine for Decision Problems are defined as follows\nA Decision Turing Machine \\(M\\) is the tuple \\((Q, \\Gamma, \\Delta, \\delta, q_\\text{init}, q_\\text{accept}, q_\\text{reject})\\) where\n\n\\(Q\\) is a finite non-empty set of states\n\\(\\Gamma\\) is an input alphabet such that \\(\\sqcup \\notin \\Gamma\\)\n\\(\\Delta\\) is a tape alphabet, satisfying the following conditions:\n\n\\(\\Gamma \\subseteq \\Delta\\)\n\\(\\sqcup \\in \\Delta\\)\n\\(\\Delta \\cap \\Gamma = \\emptyset\\)\n\n\\(q_\\text{init}, q_\\text{accept}, q_\\text{reject} \\in Q\\)\n\\(\\delta\\) (called the transition function or program) is a finite set of instructions in the form \\(q_i, s_n \\rightarrow q_j, s_m, D\\)\n\n\\(q_i, q_j \\in Q\\)\n\\(s_n, s_m \\in \\Gamma\\)\n\\(D \\in \\{ L, R \\}\\)\nand for all \\(q \\in Q \\backslash \\{q_\\text{accept}, q_\\text{reject}\\}\\) and all \\(s \\in \\Delta\\) there exists only one instruction in \\(\\delta\\) that has the form \\(q, s \\rightarrow q^\\prime, s^\\prime, D\\)\n\n\n\n\n\n\n\n\nWarningQuestion\n\n\n\nWe have seen previously that Decision Problems and Computational Problems are equivalent if an algorithm exists for one, then an algorithm must exist for the other. Try to rewrite this proof but using a Turing Machine (See Computation Problems for a definition of Computational Turing Machines) instead of a generic algorithm.\n\n\n\n\n\n\n\n\nNoteNotation\n\n\n\nDecision and Computation equivalence means that the exact specifics of our Turing Machine are not that important. We stick to a Decision Turing Machine because it is a stronger continuation from Regular Languages and Context-Free Grammers. However, there is no reason you cannot tackle this entire branch using Computational problems instead.\n\n\n\n\n3.2.3 Computing\nA configuration is the current state of our Turing Machine, where the head is positioned, and what symbols are on the tape.\nLet \\(M\\) be a Turing Machine,\n\nA configuration of \\(M\\) and tape is a word \\(w\\) in the alphabet \\(Q \\cup \\Delta\\) satisfying the following:\n\n\\(w\\) contains exactly one symbol from \\(Q\\)\n\\(w\\) does not begin with \\(\\sqcup\\)\nIf the last symbol of \\(w\\) is a \\(\\sqcup\\), the it is preceded by a symbol from \\(Q\\)\n\nA configuration is halting if it contains \\(q_\\text{accept}\\), \\(q_\\text{reject}\\)\nA configuration is accepting if it contains \\(q_\\text{accept}\\)\nA configuration is rejecting if it contains \\(q_\\text{reject}\\)\n\nOur machine starts in \\(q_\\text{init}\\) and has the input string written on its tape\nLet \\(M\\) be a Turing Machine, and \\(x \\in \\Gamma^\\ast\\),\nThe initial configuration of \\(M\\) on \\(x\\) is \\[ q_\\text{init}x \\]\nComputation is the process of moving between different configurations of our machine and tape. In fact, a computation is the sequence of configurations start from the initial configuration until it is in a halting configuration.\nLet \\(M\\) be a Turing Machine, and \\(w\\) and \\(v\\) are configurations of \\(M\\),\nWe say that \\(w\\) yields to \\(v\\) (\\(w \\vdash v\\)) if one of the following conditions hold:\n\n\\(q, s \\rightarrow, q^\\prime, s^\\prime, L \\in \\delta\\) and\n\neither \\(w = xtqsy\\) and \\(v = xq^\\prime t s^\\prime y\\) for \\(t \\in \\Delta\\) and \\(x, y \\in \\Delta^\\ast\\)\nor \\(w = qsx\\) and \\(v = q^\\prime\\sqcup s^\\prime x\\) for \\(x \\in \\Delta*\\)\n\n\\(q, s \\rightarrow, q^\\prime, s^\\prime, R \\in \\delta\\) and\n\neither \\(w = xqsy\\) and \\(v = x s^\\prime q^\\prime y\\) for \\(x, y \\in \\Delta^\\ast\\)\nor \\(w = xqs\\) and \\(v = x s^\\prime q^\\prime \\sqcup\\) for \\(x \\in \\Delta*\\)\n\n\n\n\n\n\n\n\nNote\n\n\n\nOur example in 3.2.1 produces the sequence of configurations \\[ q_0 0111 \\vdash 0 q_1 111 \\vdash 01 q_1 11 \\vdash 011 q_1 1 \\vdash 0111 q_1 \\sqcup \\vdash 0111\\sqcup q_\\text{halt} \\sqcup \\]\n\n\nLet \\(M\\) be a Turing Machine and \\(x \\in \\Gamma^\\ast\\),\nThe computation of \\(M\\) on \\(x\\) is the sequence of configurations \\(c_0, c_1, c_2, ...\\) such that\n\n\\(c_0\\) is the initial configuration of \\(M\\) on \\(x\\)\n\\(c_i \\vdash c_{i + 1}\\) for all \\(i \\geq 0\\)\nThe sequence has at most one halting configuration.\nIf the sequence has a halting configuration \\(c_n\\), then \\(c_n\\) is the last configuration in the sequence.\n\n\n\n\n\n\n\nTipNotation\n\n\n\nWe will use the symbol \\(C(M,x)\\) to refer to the computation of \\(M\\) on \\(x\\).\nThe length of a computation \\(|C(M,x)|\\) is the number of configurations in a sequence.\n\n\nWe want to identify the status of our computation\nLet \\(M\\) be a Turing Machine \\(M\\) and \\(x \\in \\Gamma^\\ast\\)\n\n\\(M\\) halts on \\(x\\) if the computation \\(C(M,x)\\) (sequence \\(c_0, c_1, c_2, ...\\)) contains a halting configuration\n\\(M\\) loops on \\(x\\) if the sequence \\(c_0, c_1, c_2, ...\\) does not contain a halting configuration\n\\(M\\) accepts \\(x\\) if the sequence \\(c_0, c_1, c_2, ...\\) contains an accepting configuration\n\\(M\\) rejects \\(x\\) if the sequence \\(c_0, c_1, c_2, ...\\) contains a rejecting configuration\n\n\n\n\n\n\n\nTipNotation\n\n\n\n\\[\n    M(x) = \\begin{cases}\n        1 & \\text{if $M$ accepts $x$} \\\\\n        0 & \\text{if $M$ rejects $x$} \\\\\n        \\infty & \\text{otherwise}\n    \\end{cases}\n\\]\n\n\nLastly, we want a way to classify TM’s based on whether they accept, reject or loop on all inputs.\nLet \\(M\\) be a Turing Machine\n\n\\(M\\) is a decider if it accepts or rejects \\(x\\) for all \\(x \\in \\Gamma^\\ast\\)\n\\(M\\) decides / solves a decision problem \\(P: \\Gamma^\\ast \\mapsto \\{ 0, 1 \\}\\) if \\(\\forall x \\in \\Gamma^\\ast, M(x) = P(x)\\)\n\n\n\n\n\n\n\nWarningQuestion\n\n\n\nLet \\(M\\) be a Turing Machine\n\nIf \\(M\\) is a decider, does it solve some decision problem?\nIf \\(M\\) solves a decision problem, is it a decider?\n\n\n\n\n\n\n\n\n\nTipNotation\n\n\n\nWe say that a Turing Machine \\(M\\) decides a language \\(L \\subseteq \\Gamma^\\ast\\) if \\[ M(x) = 1 \\iff x \\in L \\]\n\n\nTwo Turing Machines \\(M\\) and \\(N\\) on the same input alphabet \\(\\Gamma\\) are equivalent if they have the same input and output behaviour \\[ \\forall x \\in \\Gamma^\\ast, M(x) = N(x)\\]\n\n\n\n\n\n\nWarningQuestion\n\n\n\nHow many equivalent Turing Machines are there?\nProof Sketch: If I take some Turing Machine \\(M\\), we can add a new state \\(q\\) and make the new machine \\(M^\\prime\\). We can make \\(q\\) fall between any state \\(q_i\\) that has some transition to \\(q_\\text{accept}\\).\n\nIf \\(q_i\\) has a transition to \\(q_\\text{accept}\\), make it instead transition to \\(q\\) (leaving the rest of the transition unmodified)\n\\(q\\) transitions to \\(q_\\text{accept}\\) on every symbol, leaving the symbol unmodified.\n\n\\(M^\\prime\\) has not changed anything about any computation besides add a new configuration \\(c_{n + 1}\\) to the sequence. It has not changed the outcome, nor has it changed the tape in anyway. Thus \\(M^\\prime\\) is equivalent to \\(M\\).\nWe can then create \\(M^{\\prime\\prime}\\) in the exact same fashion from \\(M^\\prime\\). Repeat this ad-infinitum, and we have an infinite number of equivalent Turing Machines.\n\n\n\n\n3.2.4 Computational Problems\nA Turing Machine for computational problems is very similar to the standard definition. The only difference is that we have a single notable state \\(q_\\text{halt}\\) instead of \\(q_\\text{accept}\\) and \\(q_\\text{accept}\\)\nA Computational Turing Machine \\(M\\) is the tuple \\((Q, \\Gamma, \\Delta, \\delta, q_\\text{init}, q_\\text{halt})\\) where\n\n\\(Q\\) is a finite non-empty set of states\n\\(\\Gamma\\) is an input alphabet such that \\(\\sqcup \\notin \\Gamma\\)\n\\(\\Delta\\) is a tape alphabet, satisfying the following conditions:\n\n\\(\\Gamma \\subseteq \\Delta\\) - Our input alphabet can be written on the tape\n\\(\\sqcup \\in \\Delta\\) - Our tape has a blank symbol\n\\(\\Delta \\cap \\Gamma = \\emptyset\\) - The symbols and states are different\n\n\\(q_\\text{init}, q_\\text{halt} \\in Q\\)\n\\(\\delta\\) (called the transition function or program) is a finite set of instructions in the form \\(q_i, s_n \\rightarrow q_j, s_m, D\\)\n\n\\(q_i, q_j \\in Q\\)\n\\(s_n, s_m \\in \\Gamma\\)\n\\(D \\in \\{ L, R \\}\\)\nand for all \\(q \\in Q \\backslash \\{q_\\text{halt}\\}\\) and all \\(s \\in \\Delta\\) there exists only one instruction in \\(\\delta\\) that has the form \\(q, s \\rightarrow q^\\prime, s^\\prime, D\\)\n\n\nThe definitions of computation and status of a Computational TM remain the same as a Decision TM. However, we will use a slightly different configuration to define these objects:\n\nA configuration is halting if contains \\(q_\\text{halt}\\)\n\nComputational Turing Machines need to provide some form of output string that we can read off the tape. To be consistent, we say that our resultant string is the string from the tapes head until the next blank symbol that is composed of only input symbols \\(\\Gamma\\).\nLet \\(M\\) be a Computational Turing Machine.\nWe define the result of a computation of \\(M\\) on \\(x\\) as the string \\(w\\) satisfying the following properties\n\n\\(w \\in \\Gamma^\\ast\\)\n\\(w\\) is written on the tap\n\\(w\\) is immediately followed by \\(\\sqcup\\)\n\\(M\\) is scanning the first symbol of \\(w\\)\n\n\n\n\n\n\n\nTipNotation\n\n\n\n\\[ M(x) = \\begin{cases}\n    w & \\text{if } C(M,x) \\text{ halts} \\\\\n    \\infty & \\text{otherwise} \\\\\n\\end{cases}\n\\]\n\n\nComputational TM’s can also solve some Computational Problem \\(P: \\Gamma^\\ast \\mapsto \\Gamma^\\ast\\). However we use the term \\(M\\) computes \\(P\\) to specify it is a computational problem.\nLet \\(M\\) be a Computational Turing Machine and \\(P: \\Gamma^\\ast \\mapsto \\Gamma^\\ast\\) a computational problem\n\n\\(M\\) computes \\(P\\) if \\[ \\forall x \\in \\Gamma^\\ast, M(x) = P(x)\\]\n\nand this gives us a fairly strong definition for when something is computable or not.\nLet \\(f:\\Sigma^\\ast \\mapsto \\Sigma^\\ast\\) be a function.\n\n\\(f\\) is computable if there exists some Turing Machine that computes \\(f\\)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Turing Machines I</span>"
    ]
  },
  {
    "objectID": "lectures/lecture_3.html#turing-machines-as-algorithms",
    "href": "lectures/lecture_3.html#turing-machines-as-algorithms",
    "title": "3  Turing Machines I",
    "section": "3.3 Turing Machines as Algorithms",
    "text": "3.3 Turing Machines as Algorithms\n\n3.3.1 Analysis of Turing Machines\nWhen we perform an analysis of an algorithm, we typically focus on two resources available to our algorithm:\n\nTime Complexity - How long will it take to run our algorithm\nSpace Complexity - How much memory do we need for our algorithm\n\nIf we want to use Turing Machines as a mathematical model for our algorithms, we need to define these same tools.\n\n3.3.1.1 Time\nOur Turing Machine has only the single instruction. If we build this machine in reality, then it will take a roughly fixed amount of time to execute this instruction, no matter how many times we do it.\n\nIf we built this device using mechanical components, this time is how long it takes to rotate the gears, manipulate the linkages and so on.\nIf we built it use electronic signals, it would be the time to change the voltage of our circuit.\n\nThis means we can discard the actual unit and instead count the number of times an instruction is executed. We can recover the actual execution time by taking an average time to execute a single instruction and multiply it by the number of instructions.\nA single instruction allows us to move between the different configurations of our machine, thus the number of instructions is the number of configurations in a computation.\nLet \\(M\\) be a Turing Machine and \\(x \\in \\Gamma^\\ast\\),\nThe computation time of \\(M\\) on \\(x\\) is the length of the computation.\n\\[ T(M,x) = |C(M,x)| \\]\n\\(T(M,x)\\) allows us to analyse the execution of a single input, but we want to understand how it runs on all inputs. However, if two strings are of equal length, then the machine will do the roughly the same amount of work, even if the result of that work is different. Instead of the individual strings we analyse the length of the strings.\nLet \\(M\\) be a Turing Machine that will halt on every input,\nThe time function \\(t: \\mathbb{N} \\mapsto \\mathbb{N}\\) of \\(M\\) is \\[ t(n) = \\max \\{ T(M,x) = |C(M,x)| : x \\in \\Gamma^n \\} \\]\nIn english, our time function for a given input \\(n\\) is the maximum computation time across all strings of length \\(n\\).\n\n\n\n\n\n\nNote\n\n\n\nWe use the maximum time because some inputs of the same length may occasionally do less work.\nConsider the following python script\nx = input()\nfor i in range(0, len(x))\n    if x[i] == \"0\":\n        break\nIf we input “111”, the for loop will be executed 3 times. If we instead input “101” it will only be executed twice, even though the strings are the same length.\nWhen analysing this algorithm, we want to know the worst it will do at a given length, not the best. So we take \\(t(3) = \\max\\{ 1, 2, 3, 3 \\} = 3\\) (Inputs: \\(\\{ 0xx, 10x, 110, 111 \\}\\) where \\(x \\in \\Gamma\\))\n\n\n\n\n3.3.1.2 Space\nIn a digital computer, space is measured in the number of bits used. We can do the same here, but instead of bits we will use our tape instead of transistors.\nLet \\(M\\) be a Turing Machine and \\(x \\in \\Gamma^\\ast\\),\nThe computation space \\(S(M,x)\\) of \\(M\\) on \\(x\\) is the number of unique tape cells accessed by our machine.\nThis is more difficult to tie down an exact value, but we can define some bounds.\nThe machine can only move left or right on an instruction, it cannot stay in place. Even if we use the empty string, the machine scans a cell in the initial configuration, and scans the next cell after the instruction that may let it halt.\n\\[ q_\\text{init}, \\sqcup \\rightarrow q_\\text{halt / accept / reject}, s, D \\]\nFor any other string, we can do the same, or just move back and forth between these two cells infinitely. So the smallest number of unique cells we can use is 2. And if we never re-use a cell i.e. every instruction accesses a new cell, then we can only access as many cells as there are instructions.\n\\[ 2 \\leq S(M,x) \\leq T(M,x) \\]\nFor the same reasons as time, we want to study the lengths of a string and how the machine behaves, so we define our space function in a similar manner.\nLet \\(M\\) be a Turing Machine that will halt on every input,\nThe space function \\(s: \\mathbb{N} \\mapsto \\mathbb{N}\\) of \\(M\\) is \\[ s(n) = \\max \\{ S(M,x) : x \\in \\Gamma^n \\} \\]\n\n\n\n3.3.2 The Church-Turing Thesis\nAt this point, you should have an understanding that Turing Machines are a fairly strong model for algorithms. We can analyse them the same way as algorithms, we can program them using instructions, we can build them in the real world (this is a surprisingly important idea), and there is more yet to come.\nHowever, we have not really discussed if a Turing Machine is an exact model of an algorithm. To jump the gun, yes. Turing Machines are an incredibly robust model, and every definition of computer that has been developed in the last 100 years has been shown to be equivalent to at least some form of Turing Machine. Even digital computers with Random-Access Memory are just slightly faster versions of a Turing Machine. In Turing Equivalency, we will study this idea more concretely.\nTo formulate this, we have developed a simple thesis.\n\nEvery computational problem solvable by an algorithm, can also be solved by a Turing Machine.\n\nThis is the Church-Turing Thesis. It allows us to assert that an algorithm is a Turing Machine. If we can’t write a Turing Machine, then we can’t write an algorithm and vice versa.\nThis is not a mathematical statement however. It converts an intuitive concept (an algorithm - sequence of steps) and converts it into a mathematical object (a TM - sequence of instructions). We cannot really prove this assertion, we only take it as true so long as it appears to be true.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Turing Machines I</span>"
    ]
  },
  {
    "objectID": "lectures/lecture_3.html#footnotes",
    "href": "lectures/lecture_3.html#footnotes",
    "title": "3  Turing Machines I",
    "section": "",
    "text": "Arthur C. Clarke’s third law↩︎\ncitation needed↩︎\nOne day I will find citations for this…↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Turing Machines I</span>"
    ]
  }
]